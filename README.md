# Sistema de AnotaciÃ³n de Video

**Proyecto Final - Inteligencia Artificial 1**

Un sistema completo para la detecciÃ³n y clasificaciÃ³n de actividades humanas en tiempo real utilizando MediaPipe y tÃ©cnicas de aprendizaje automÃ¡tico.

## ğŸ“‹ DescripciÃ³n del Proyecto

Este sistema permite analizar videos en tiempo real para detectar y clasificar las siguientes actividades:

- **Caminar hacia la cÃ¡mara**: Persona se acerca frontalmente
- **Caminar alejÃ¡ndose**: Persona se aleja de espaldas
- **Girar 90 grados**: Un cuarto de vuelta a la izquierda o derecha
- **Girar 180 grados**: Media vuelta completa
- **Sentarse**: Desde de pie a sentado en silla
- **Ponerse de pie**: Desde sentado a de pie

### CaracterÃ­sticas Principales

- **DetecciÃ³n de poses en tiempo real** usando MediaPipe
- **ClasificaciÃ³n de actividades** con mÃºltiples algoritmos de ML
- **AnÃ¡lisis de inclinaciones laterales** y movimientos articulares
- **Interfaz grÃ¡fica intuitiva** para visualizaciÃ³n en tiempo real
- **Pipeline completo** desde extracciÃ³n hasta entrenamiento
- **AnÃ¡lisis exploratorio** de datos con visualizaciones

## ğŸ—ï¸ Arquitectura del Sistema

```
sistema_anotacion_video/
â”œâ”€â”€ src/                          # CÃ³digo fuente principal
â”‚   â”œâ”€â”€ __init__.py              # ConfiguraciÃ³n del mÃ³dulo
â”‚   â”œâ”€â”€ main.py                  # Punto de entrada principal
â”‚   â”œâ”€â”€ pose_detection/          # DetecciÃ³n de poses con MediaPipe
â”‚   â”œâ”€â”€ activity_classifier/     # ClasificaciÃ³n de actividades
â”‚   â”œâ”€â”€ gui/                     # Interfaz grÃ¡fica de usuario
â”‚   â”œâ”€â”€ video_capture/          # Captura y manejo de video
â”‚   â””â”€â”€ utils/                   # Utilidades auxiliares
â”œâ”€â”€ videos/                      # Videos de entrenamiento por actividad
â”œâ”€â”€ data/                        # Datos procesados y caracterÃ­sticas
â”œâ”€â”€ models/                      # Modelos entrenados y evaluaciones
â”œâ”€â”€ analysis/                    # AnÃ¡lisis exploratorio de datos
â”œâ”€â”€ extract_video_features.py   # ExtracciÃ³n de caracterÃ­sticas
â”œâ”€â”€ train_classifier.py         # Entrenamiento de modelos
â”œâ”€â”€ data_analysis.py            # AnÃ¡lisis exploratorio
â”œâ”€â”€ run_pipeline.py             # Pipeline completo automatizado
â””â”€â”€ requirements.txt            # Dependencias del proyecto
```

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerequisitos

- Python 3.8 o superior
- CÃ¡mara web funcional
- Sistema operativo: Windows, macOS, o Linux

### InstalaciÃ³n

1. **Clonar el repositorio**:
```bash
git clone <url-del-repositorio>
cd sistema_anotacion_video
```

2. **Crear entorno virtual** (recomendado):
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. **Instalar dependencias**:
```bash
pip install -r requirements.txt
```

4. **Verificar instalaciÃ³n**:
```bash
python src/main.py
```

## ğŸ“¹ PreparaciÃ³n de Datos

### Estructura de Videos

Los videos de entrenamiento deben organizarse en la siguiente estructura:

```
videos/
â”œâ”€â”€ caminar_hacia/
â”‚   â”œâ”€â”€ video1.mp4
â”‚   â”œâ”€â”€ video2.mp4
â”‚   â””â”€â”€ video3.mp4
â”œâ”€â”€ caminar_regreso/
â”œâ”€â”€ girar_90/
â”œâ”€â”€ girar_180/
â”œâ”€â”€ sentarse/
â””â”€â”€ ponerse_de_pie/
```

### Recomendaciones para GrabaciÃ³n

âœ… **Hacer**:
- Persona completa visible en el frame
- Buena iluminaciÃ³n (evitar sombras fuertes)
- Fondo despejado
- CÃ¡mara estable
- DuraciÃ³n: 5-15 segundos por video
- MÃ­nimo 3 videos por actividad

âŒ **Evitar**:
- Personas parcialmente cortadas
- IluminaciÃ³n muy pobre
- Movimientos muy rÃ¡pidos
- MÃºltiples personas en el frame

## ğŸ”„ Pipeline de Entrenamiento

### OpciÃ³n 1: Pipeline Automatizado (Recomendado)

Ejecutar todo el proceso de una vez:

```bash
# Pipeline completo
python run_pipeline.py

# Pipeline rÃ¡pido (sin Grid Search)
python run_pipeline.py --quick_run

# Para pruebas con pocos datos
python run_pipeline.py --max_videos 2 --max_frames 100 --quick_run
```

### OpciÃ³n 2: EjecuciÃ³n Manual por Pasos

#### Paso 1: ExtracciÃ³n de CaracterÃ­sticas
```bash
python extract_video_features.py
```

#### Paso 2: AnÃ¡lisis Exploratorio
```bash
python data_analysis.py
```

#### Paso 3: Entrenamiento de Modelos
```bash
python train_classifier.py
```

## ğŸ¯ Uso del Sistema

### AplicaciÃ³n Principal

Ejecutar la interfaz grÃ¡fica:
```bash
python src/main.py
```

### Funcionalidades de la GUI

- **Vista en tiempo real** de la cÃ¡mara con detecciÃ³n de poses
- **ClasificaciÃ³n automÃ¡tica** de actividades
- **VisualizaciÃ³n de Ã¡ngulos articulares** y inclinaciones
- **Ajuste de umbrales** de detecciÃ³n en tiempo real
- **Historial de actividades** detectadas
- **Pistas visuales** para guiar al usuario

## ğŸ“Š AnÃ¡lisis y Resultados

### AnÃ¡lisis Exploratorio

Los resultados del anÃ¡lisis se guardan en:
- `analysis/reports/analysis_report.txt` - Reporte completo
- `analysis/plots/` - Visualizaciones y grÃ¡ficos
- `analysis/statistics/` - EstadÃ­sticas detalladas

### EvaluaciÃ³n de Modelos

Los resultados del entrenamiento se encuentran en:
- `models/evaluation/model_comparison.csv` - ComparaciÃ³n de modelos
- `models/evaluation/best_model_info.json` - InformaciÃ³n del mejor modelo
- `models/plots/` - GrÃ¡ficos de evaluaciÃ³n

### MÃ©tricas Principales

El sistema evalÃºa los modelos usando:
- **Accuracy**: PrecisiÃ³n general
- **Precision**: PrecisiÃ³n por clase
- **Recall**: Sensibilidad por clase
- **F1-Score**: Media armÃ³nica de precisiÃ³n y recall

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Modelos Disponibles

El sistema entrena mÃºltiples algoritmos:
- **Random Forest**: Robusto y rÃ¡pido
- **SVM**: Efectivo para datos de alta dimensiÃ³n
- **Logistic Regression**: Simple y interpretable
- **Gradient Boosting**: Potente para patrones complejos
- **K-Nearest Neighbors**: Bueno para decisiones locales
- **XGBoost**: Opcional, requiere instalaciÃ³n adicional

### PersonalizaciÃ³n de ParÃ¡metros

Puedes ajustar parÃ¡metros en los scripts:

```bash
# Seleccionar nÃºmero de caracterÃ­sticas
python train_classifier.py --n_features 30

# Cambiar validation folds
python train_classifier.py --cv_folds 3

# Usar archivo especÃ­fico de datos
python train_classifier.py --data_file mi_dataset.csv
```

## ğŸ“ˆ MetodologÃ­a CRISP-DM

El proyecto sigue la metodologÃ­a CRISP-DM:

1. **ComprensiÃ³n del Negocio**: DetecciÃ³n de actividades humanas
2. **ComprensiÃ³n de Datos**: AnÃ¡lisis de videos y landmarks
3. **PreparaciÃ³n de Datos**: ExtracciÃ³n y normalizaciÃ³n de caracterÃ­sticas
4. **Modelado**: Entrenamiento de mÃºltiples algoritmos
5. **EvaluaciÃ³n**: MÃ©tricas de rendimiento y validaciÃ³n cruzada
6. **Despliegue**: AplicaciÃ³n en tiempo real

## ğŸ¤– Aspectos TÃ©cnicos

### DetecciÃ³n de Poses

- **MediaPipe Pose**: DetecciÃ³n de 33 landmarks corporales
- **NormalizaciÃ³n**: Coordenadas relativas al centro de cadera
- **Filtrado**: Suavizado de landmarks para reducir ruido

### ExtracciÃ³n de CaracterÃ­sticas

- **Coordenadas de landmarks**: x, y, z normalizadas
- **Ãngulos articulares**: Rodillas, caderas, codos
- **CaracterÃ­sticas geomÃ©tricas**: Distancias y proporciones
- **CaracterÃ­sticas temporales**: Velocidades y aceleraciones

### ClasificaciÃ³n

- **Enfoque hÃ­brido**: Reglas + Machine Learning
- **CaracterÃ­sticas seleccionadas**: Top N mÃ¡s relevantes
- **ValidaciÃ³n cruzada**: EvaluaciÃ³n robusta
- **OptimizaciÃ³n**: Grid Search para hiperparÃ¡metros

## ğŸ” ResoluciÃ³n de Problemas

### Problemas Comunes

1. **Error de cÃ¡mara**:
   - Verificar que la cÃ¡mara no estÃ© en uso
   - Probar con diferentes Ã­ndices de cÃ¡mara

2. **Baja precisiÃ³n**:
   - Agregar mÃ¡s videos de entrenamiento
   - Mejorar calidad de iluminaciÃ³n
   - Ajustar umbrales de detecciÃ³n

3. **Rendimiento lento**:
   - Reducir resoluciÃ³n de video
   - Usar modo rÃ¡pido (--quick_run)
   - Cerrar otras aplicaciones

### Logs y DepuraciÃ³n

- Los logs se muestran en consola durante la ejecuciÃ³n
- Los errores se guardan en archivos de salida
- Usar `--verbose` para mÃ¡s informaciÃ³n detallada

## ğŸ¤ ContribuciÃ³n

### Desarrollo

Para contribuir al proyecto:

1. Fork del repositorio
2. Crear rama para nueva funcionalidad
3. Implementar cambios con tests
4. Hacer pull request

### Estructura de CÃ³digo

- **DocumentaciÃ³n**: Docstrings en todas las funciones
- **Estilo**: Seguir PEP 8
- **Testing**: Agregar tests para nuevas funcionalidades

## ğŸ“š Referencias y Bibliografia

### TecnologÃ­as Utilizadas

- [MediaPipe](https://ai.google.dev/edge/mediapipe/solutions/guide) - DetecciÃ³n de poses
- [OpenCV](https://opencv.org/) - Procesamiento de video
- [scikit-learn](https://scikit-learn.org/) - Machine Learning
- [PyQt5](https://pypi.org/project/PyQt5/) - Interfaz grÃ¡fica

### ArtÃ­culos de Referencia

- Pose Detection and Activity Recognition using MediaPipe
- Human Activity Recognition: A Comprehensive Survey
- Real-time Activity Classification in Video Streams

## ğŸ“„ Licencia

Este proyecto se desarrolla como parte del curso de Inteligencia Artificial 1.

---

## ğŸ‘¥ Equipo de Desarrollo

**Proyecto Final - IA1**
- AnÃ¡lisis de actividades humanas en tiempo real
- Sistema completo de detecciÃ³n y clasificaciÃ³n
- Interfaz grÃ¡fica para uso interactivo

---

*Para mÃ¡s informaciÃ³n o soporte, consultar la documentaciÃ³n en el cÃ³digo fuente o contactar al equipo de desarrollo.*
